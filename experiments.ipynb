{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "import transformers\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>full_text</th>\n",
       "      <th>score</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000d118</td>\n",
       "      <td>Many people have car where they live. The thin...</td>\n",
       "      <td>3</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000fe60</td>\n",
       "      <td>I am a scientist at NASA that is discussing th...</td>\n",
       "      <td>3</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001ab80</td>\n",
       "      <td>People always wish they had the same technolog...</td>\n",
       "      <td>4</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>001bdc0</td>\n",
       "      <td>We all heard about Venus, the planet without a...</td>\n",
       "      <td>4</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>002ba53</td>\n",
       "      <td>Dear, State Senator\\n\\nThis is a letter to arg...</td>\n",
       "      <td>3</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  essay_id                                          full_text  score  \\\n",
       "0  000d118  Many people have car where they live. The thin...      3   \n",
       "1  000fe60  I am a scientist at NASA that is discussing th...      3   \n",
       "2  001ab80  People always wish they had the same technolog...      4   \n",
       "3  001bdc0  We all heard about Venus, the planet without a...      4   \n",
       "4  002ba53  Dear, State Senator\\n\\nThis is a letter to arg...      3   \n",
       "\n",
       "                label  \n",
       "0  [0, 0, 1, 0, 0, 0]  \n",
       "1  [0, 0, 1, 0, 0, 0]  \n",
       "2  [0, 0, 0, 1, 0, 0]  \n",
       "3  [0, 0, 0, 1, 0, 0]  \n",
       "4  [0, 0, 1, 0, 0, 0]  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/train.csv')\n",
    "df['label'] = df['score'].apply(lambda num: [1 if i == num - 1 else 0 for i in range(6)])\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train = Dataset.from_pandas(train)\n",
    "test = Dataset.from_pandas(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['essay_id', 'full_text', 'score', 'label', '__index_level_0__'],\n",
       "    num_rows: 3462\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonat/virtual_enviornments/data_env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 13845/13845 [00:08<00:00, 1680.19 examples/s]\n",
      "Map: 100%|██████████| 3462/3462 [00:01<00:00, 1978.83 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "PRE_TRAINED_MODEL_NAME = 'xlm-roberta-base'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['full_text'], padding=True, truncation=True)\n",
    "tokenized_train = train.map(tokenize, batched=True, batch_size=len(train))\n",
    "tokenized_test = test.map(tokenize, batched=True, batch_size=len(test))\n",
    "\n",
    "tokenized_dict = DatasetDict({'train': tokenized_train, 'test': tokenized_test})\n",
    "tokenized_dict.set_format(type='torch', columns=['input_ids', 'attention_mask', 'score','label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_v0(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(model_v0, self).__init__()\n",
    "        self.model = AutoModel.from_pretrained(PRE_TRAINED_MODEL_NAME, torch_dtype=torch.float32)\n",
    "        self.linear = torch.nn.Linear(768, 6)\n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        output = self.model(input_ids=input['input_ids'].to(device), attention_mask=input['attention_mask'].to(device))\n",
    "        output = output.last_hidden_state[:,0,:].to(torch.float32)\n",
    "        return self.softmax(self.linear(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(tokenized_dict['train'], batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(tokenized_dict['test'], batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loss_fn, optimizer, train_loader, val_loader, epochs=3,testing=False):\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        loss_train = 0.0\n",
    "        for batch in tqdm.tqdm(train_loader, desc=f'Epoch {epoch} of {epochs}', total=len(train_loader)):\n",
    "            target = batch['label'].to(device).to(torch.float32)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch)\n",
    "            loss = loss_fn(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_train += loss.item()\n",
    "        loss_train = loss_train/len(train_loader)\n",
    "        #evaluation\n",
    "        if testing:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                loss_val = 0.0\n",
    "                score = 0.0\n",
    "                for batch in tqdm.tqdm(val_loader, desc=f'Testing: epoch {epoch} of {epochs}', total=len(val_loader)):\n",
    "                    target = batch['label'].to(device).to(torch.float32)\n",
    "                    output = model(batch)\n",
    "                    loss = loss_fn(output, target)\n",
    "                    loss_val += loss.item()\n",
    "                    pred = torch.argmax(output, dim=1).cpu().numpy()\n",
    "                    target = torch.argmax(target, dim=1).cpu().numpy()\n",
    "                    score += cohen_kappa_score(pred,target, weights='quadratic')\n",
    "                loss_val = loss_val/len(val_loader)\n",
    "                score = score/len(val_loader)\n",
    "            print(f'Epoch: {epoch}, Training Loss: {loss_train}, Validation Loss: {loss_val} | Cohen Kappa Score: {score}')\n",
    "        else:\n",
    "            print(f'Epoch: {epoch}, Training Loss: {loss_train}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = tokenized_dict['train'].select(range(10))\n",
    "subset_loader = DataLoader(subset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 1: 100%|██████████| 2/2 [04:00<00:00, 120.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss: 1.7902097702026367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = model_v0().to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-6)\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "train(model, loss, optimizer, subset_loader, test_loader,epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      3\u001b[0m loss_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_loader:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "model.train()\n",
    "loss_train = 0\n",
    "for batch in train_loader:\n",
    "    target = batch['label'].to(device).to(torch.float16)\n",
    "    optimizer.zero_grad()\n",
    "    output = model(batch)\n",
    "    loss = loss_fn(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    loss_train += loss.item()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = torch.argmax(output, dim=1).cpu().numpy()\n",
    "target = torch.argmax(target, dim=1).cpu().numpy()\n",
    "score = cohen_kappa_score(pred,target, weights='quadratic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
